{"title":"Regression Wisdom","markdown":{"yaml":{"title":"Regression Wisdom","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"1/26/2022","output":{"learnr::tutorial":{"toc_depth":2}},"runtime":"shiny_prerendered"},"headingText":"Regression issues","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nknitr::opts_chunk$set(warning = FALSE)\nknitr::opts_chunk$set(message = FALSE)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(gridExtra)\nlibrary(broom)\nlibrary(learnr)\n\ntheme_set(theme_classic())\nset.seed(20200915)\n\nauto.mpg <- read.csv(\"www/auto.mpg.csv\")\nclassroster <- read.csv(\"www/classroster.csv\", fileEncoding=\"UTF-8-BOM\")\n\nmod.dis <- lm(mpg ~ displacement, data=auto.mpg)\nmod <- lm(data=auto.mpg, mpg ~ horsepower, na.action = na.exclude)\n\nshoesize <- rnorm(20, mean=43, sd=1.5)\niq <- rnorm(20, mean=100, sd=15)\nshoeiq<-data.frame(shoesize, iq)\n\nshoesize.lb<-c(shoesize,52)\niq.lb<-c(iq,150)\nshoeiq.lb<-data.frame(shoesize.lb, iq.lb)\n\nshoesize.bg<-c(shoesize,43)\niq.bg<-c(iq,150)\nshoeiq.bg<-data.frame(shoesize.bg, iq.bg)\n\nshoesize.rw<-c(shoesize,52)\niq.rw<-c(iq,100)\nshoeiq.rw<-data.frame(shoesize.rw, iq.rw)\n```\n\n\n* Model fit measures\n* Extrapolation\n* Outliers\n  + Leverage\n  + Influence\n* Interpreting a regression\n\n## Model fit measures\n\n### Regression results\n\nMany parts of these results we already know how to interpret. For now, we will focus on model fit measures.\n\n- $R^2$\n- $s_e$\n- Residuals 5 number summary\n\n```{r basicmodel, exercise=TRUE}\nmod.dis <- lm(mpg ~ displacement, data=auto.mpg)\nsummary(mod.dis)\n```\n\n### Residuals - histogram\n\n```{r basicmodelresids, exercise=TRUE}\nauto.mpg.augment <- augment(mod.dis, auto.mpg)\n\nggplot(auto.mpg.augment, aes(x=.resid)) +\n  geom_histogram(fill=\"blue4\", color=\"black\") + \n  labs(y = \"Count\", x=\"Residual size\") +\n  geom_vline(xintercept=0, color=\"red\")\n```\n\nHow would you interpret this residual histogram?\n\n```{r picker1, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Correlation review\n\n* Recall that a correlation, $r$, ranges from -1 to 1 and indicates the strength of the association between two variables\n  + If $r$ is -1 or 1 exactly, there is no variation, the correlation indicate the relationship is a straight line\n  + Note that $r$ does not indicate the slope\n  + If $r$ is 0, that means there is no relationship\n    - What is the slope in that case?\n    - $\\hat{y} = b_0 + b_1*x$\n\n```{r picker2, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### R squared definition\n\n* $R^2$ is the square of $r$ in a two variable case, so between 0 and 1\n\n* But, unlike $r$, $R^2$ is meaningful in multivariate models\n\n* Percent of the total variation in the data explained by the model\n\n* Sum of the errors from our model divided by sum of errors from the 'braindead' model of $\\hat{y}=\\bar{y}$\n\n* If the $R^2$ is small, that means our model doesn't beat the 'braindead' model by very much\n\n```{r braindead, exercise=TRUE}\nggplot(auto.mpg, aes(x=displacement, y=mpg)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  geom_hline(yintercept=mean(auto.mpg$mpg), color=\"red\")\n```\n\n### When is R squared \"big enough\"?\n\n* $R^2$ is useful, but only so much so\n\n* The closer $R^2$ is to 1, the more useful the model\n  + How close is \"close\"?\n  + Depends on the situation\n  + $R^2$ of 0.5 might be very bad for a model that uses height to predict weight\n    - Should be more closely related\n  + $R^2$ of 0.5 might be very good for a model using test scores to predict future income\n    - Response variable has a lot of factors that shape it and a lot of noise\n\n* **Good practice**: always report $R^2$ and $s_e$ and let readers analyze the results as well\n\n## Extrapolating\n\n* The farther a new value is from the range of x, the less trust we should place in the predicted value of y\n\n* Venture into new x territory, called extrapolation\n\n* **Dubious**: questionable assumption that nothing changes about the relationship between x and y changes for extreme values of x\n\n### Predicting MPG of cars\n\n1970s data on automobiles\n\n```{r weightmodel, exercise=TRUE}\nsummary(lm(data=auto.mpg, mpg ~ weight), digits=3)\n```\n\n### Predicting the Maybach\n\n```{r maybachexterior, out.width = \"400px\", fig.cap=\"World's heaviest car\"}\nknitr::include_graphics(\"www/maybach.exterior.jpg\")\n```\n\n### Predicting the Maybach\n\n```{r maybachinterior, out.width = \"400px\", fig.cap=\"World's heaviest car\"}\nknitr::include_graphics(\"www/maybach.interior.jpg\")\n```\n\n### Predicting the Maybach\n\n```{r maybachnk, out.width = \"400px\", fig.cap=\"World's heaviest car\"}\nknitr::include_graphics(\"www/maybach.nk.jpg\")\n```\n\nWill our model do a good job predicting this car's miles per gallon?\n\n```{r picker3, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Can we predict this car's MPG using our model?\n\nWeight: 6581 pounds  \n  \nModel: \n$\\hat{y} = b_0 + b_1*x$  \n$\\hat{y} = 46.3 + -0.00767*6581$  \n$\\hat{y} = -4.17$ miles per gallon\n\n* Nonsense prediction\n\n### Be wary of out of sample predictions\n\n```{r weightmodelgraph, exercise=TRUE}\nggplot(auto.mpg, aes(x=weight, y=mpg)) +\n  geom_point() +\n  geom_point(aes(x=6581, y=-4.17), color=\"red\") +\n  labs(x=\"Weight\", y=\"Miles per gallon\", title=\"Miles per gallon as a function of weight\")\n```\n\n## Outliers\n\n### Shoe size and IQ\n\nFirst, we can create random data for both a variable called `shoesize` and one called `iq` but in the below example they are defined to be random and have no relation to each other.\n\n```{r basicshoemodel, exercise=TRUE}\n#shoesize <- rnorm(20, mean=43, sd=1.5)\n#iq <- rnorm(20, mean=100, sd=15)\n#shoeiq<-data.frame(shoesize, iq)\n```\n\n```{r basicshoemodelgraph, exercise=TRUE}\nggplot(shoeiq, aes(x=shoesize, y=iq)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(x=\"Shoe size\", y=\"IQ\", title=\"IQ as a function of shoe size\") +\n  annotate(geom=\"text\", x=43, y=120, label=\"Slope is 2.77, r2 is 0.025\", color=\"red\")\n```\n\n### Adding LeBron James\n\n```{r lebron, out.width = \"400px\"}\nknitr::include_graphics(\"www/lebron.jpg\")\n```\n\n```{r lebronmodel, exercise=TRUE}\n#shoesize.lb<-c(shoesize,52)\n#iq.lb<-c(iq,150)\n#shoeiq.lb<-data.frame(shoesize.lb, iq.lb)\n```\n\nWhat will happen to the slope and $R^2$?\n\n```{r picker4, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Lebron plot\n\n```{r lebronplot, exercise=TRUE}\nggplot(shoeiq.lb, aes(x=shoesize.lb, y=iq.lb)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(x=\"Shoe size\", y=\"IQ\", title=\"IQ as a function of shoe size with Lebron James\") +\n  annotate(geom=\"text\", x=46, y=100, label=\"Slope is 4.85, r2 is 0.32\", color=\"red\")\n```\n\n### Adding Bill Gates\n\n```{r gates, out.width = \"400px\"}\nknitr::include_graphics(\"www/billgates.jpg\")\n```\n\n```{r billgatesmodel, exercise=TRUE}\n#shoesize.bg<-c(shoesize,43)\n#iq.bg<-c(iq,150)\n#shoeiq.bg<-data.frame(shoesize.bg, iq.bg)\n```\n\n```{r billgatesplot, exercise=TRUE}\nggplot(shoeiq.bg, aes(x=shoesize.bg, y=iq.bg)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(x=\"Shoe size\", y=\"IQ\", title=\"IQ as a function of shoe size with Bill Gates\") +\n  annotate(geom=\"text\", x=42, y=130, label=\"Slope is 3.4, r2 is 0.03\", color=\"red\")\n```\n\n### Adding Russell Westbrook\n\n```{r westbrook, out.width = \"400px\"}\nknitr::include_graphics(\"www/westbrook.jpg\")\n```\n\n```{r westbrookmodel, exercise=TRUE}\n#shoesize.rw<-c(shoesize,52)\n#iq.rw<-c(iq,100)\n#shoeiq.rw<-data.frame(shoesize.rw, iq.rw)\n```\n\n```{r westbrookplot, exercise=TRUE}\nggplot(shoeiq.rw, aes(x=shoesize.rw, y=iq.rw)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(x=\"Shoe size\", y=\"IQ\", title=\"IQ as a function of shoe size with Russell Westbrook\") +\n  annotate(geom=\"text\", x=45, y=110, label=\"Slope is 0.34, r2 is 0.00\", color=\"red\")\n```\n\n### Leverage vs. influence\n\n* A data point whose `x` value is far from the mean of the rest of the `x` values is said to have high leverage.\n\n* Leverage points have the potential to strongly pull on the regression line. \n\n* A point is influential if omitting it from the analysis changes the model enough to make a meaningful difference.\n\n* Influence is determined by\n  + The residual \n  + The leverage\n\n### Warnings\n\n* Influential points can hide in plots of residuals.\n\n* Points with high leverage pull line close to them, so have small residuals.\n\n* See points in scatterplot of original data.\n\n* Find regression model with and without the points.\n\n## Interpreting a regression\n\n### Step 1: develop some expectations\n\nHorsepower vs. MPG\n\n* More powerful engines probably are less fuel efficient\n\n* Relationship is likely roughly linear\n\n* The exact relationship depends on the efficiency of the engine\n  + Could be noisy\n\n### Step 2: make a picture\n\n```{r horsepowerplot, exercise=TRUE}\nggplot(auto.mpg, aes(x=horsepower, y=mpg)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(x=\"Horsepower\", y=\"Miles per gallon\", title=\"Miles per gallon as a function of horsepower\")\n```\n\n### Step 3: check the conditions\n\n* Quantitative variable condition\n\n* Straight enough condition\n\n* Outlier condition\n\n* Does the plot thicken\n\n* **Conclusion:**?\n\n```{r picker5, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Step 4: identify the units\n\n* Miles per gallon: amount of miles you can travel on one gallon of gas, a measure of efficiency. \n  + Most gasoline-using cars have MPG between 10-40, higher being better\n  \n* Horsepower: power of the engine. \n  + Typical values for standard cars are in the 100-200 range, higher meaning more powerful\n\n### Step 5: intepret the slope of the regression line\n\n```{r horsepowerregsummary, exercise=TRUE}\nlm(data=auto.mpg, mpg ~ horsepower) %>%\n  tidy() %>%\n  select(c(term, estimate)) %>%\n  kable(digits=3, col.names=c(\"Term\", \"Estimate\"))\n```\n\n* For every one unit increase in horsepower, miles per gallon decreases by about -0.15 units\n  + Is that a lot or a little?\n\n```{r picker6, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Step 6: determine reasonable values for the predictor variable\n\n```{r horsepowermodunits, exercise=TRUE}\nauto.mpg %>%\n  summarize(min = min(horsepower, na.rm=TRUE), q1 = quantile(horsepower, p=0.25, na.rm=TRUE), median=median(horsepower, na.rm=TRUE), q3 = quantile(horsepower, p=0.75, na.rm=TRUE), max(horsepower, na.rm=TRUE)) %>%\n  kable(col.names=c(\"Min\", \"Q1\", \"Median\", \"Q3\", \"Max\"))\n```\n\n### Step 7: interpret the intercept\n\n```{r horsepowermodintercept, exercise=TRUE}\nlm(data=auto.mpg, mpg ~ horsepower) %>%\n  tidy() %>%\n  select(c(term, estimate)) %>%\n  kable(digits=3, col.names=c(\"Term\", \"Estimate\"))\n```\n\n### Step 8: solve for reasonable predictor values\n\nHorsepower = Q1 = 75:  \n\n$\\hat{y} = b_0 + b_1*x$  \n$\\hat{y} = 39.94 + -0.158*100$  \n$\\hat{y} = 28.09$  \n\nHorsepower = Median = 93.5:  \n\n$\\hat{y} = b_0 + b_1*x$  \n$\\hat{y} = 39.94 + -0.158*100$  \n$\\hat{y} = 25.17$  \n\nHorsepower = Q3 = 126:  \n\n$\\hat{y} = b_0 + b_1*x$  \n$\\hat{y} = 39.94 + -0.158*100$  \n$\\hat{y} = 20.03$\n\n### Step 9: interpret the residuals and identify their units\n\n```{r horsepowerresids, exercise=TRUE}\nmod <- lm(data=auto.mpg, mpg ~ horsepower, na.action = na.exclude)\nauto.mpg.augment <- augment(mod, auto.mpg)\n\nggplot(auto.mpg.augment, aes(x=horsepower, y=.resid)) +\n  geom_point() + \n  geom_hline(yintercept = 0, color = \"blue\", linetype='dashed') + \n  labs(y = \"Residual error\", x=\"Horsepower\")\n```\n\n### Step 10: view the distribution of the residuals\n\n```{r horsepowerresidsdist, exercise=TRUE}\nauto.mpg.augment <- augment(mod, auto.mpg)\n\nggplot(auto.mpg.augment, aes(x=.resid)) +\n  geom_histogram(fill=\"blue4\") + \n  labs(x = \"Residual size\", y=\"Count\", title=\"Residuals from a model of MPG as a function of HP\")\n```\n\n### Step 11: interpret the residual standard error\n\n```{r horsepowerresidsse, exercise=TRUE}\nsummary(mod, digits=3)\n```\n\n### Step 12: interpret the R squared\n\n```{r horsepowerrsquared, exercise=TRUE}\nsummary(mod, digits=3)\n```\n\n### Step 13: think about confounders\n\n* What are some confounders, or \"lurking variables\"?\n  + Categorical\n  + Quantitative\n\n```{r picker7, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"learnr::tutorial":{"toc_depth":2}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Regression Wisdom.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.36","editor":"visual","theme":"cosmo","title":"Regression Wisdom","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"1/26/2022","runtime":"shiny_prerendered"},"extensions":{"book":{"multiFile":true}}}}}