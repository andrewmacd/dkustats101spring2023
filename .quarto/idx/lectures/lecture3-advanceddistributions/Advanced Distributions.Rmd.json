{"title":"Advanced Distributions","markdown":{"yaml":{"title":"Advanced Distributions","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"1/17/2022","output":{"learnr::tutorial":{"toc_depth":2}},"runtime":"shiny_prerendered"},"headingText":"More on distributions","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nknitr::opts_chunk$set(warning = FALSE)\nknitr::opts_chunk$set(message = FALSE)\n\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(learnr)\n\ntheme_set(theme_classic())\n\ntitanic_survival <- read.csv(\"www/titanic_survival.csv\")\nclassroster <- read.csv(\"www/classroster.csv\", fileEncoding=\"UTF-8-BOM\")\nnbaboxscores <- read.csv(\"www/nbaboxscores.csv\")\n\nwestern_conf <- c(\"DAL\", \"DEN\", \"GSW\", \"HOU\", \"LAC\", \"LAL\", \"MEM\", \"MIN\", \"NOP\", \"OKC\", \"PHX\", \"POR\", \"SAC\", \"SAS\", \"UTA\")\ndivisions <- as.factor(c(\"West\", \"East\"))\n\nnbaboxscores <- nbaboxscores %>%\n  mutate(W.L = factor(nbaboxscores$W.L, labels = c(\"Loss\", \"Win\")))\n\nnbaboxscores <- nbaboxscores %>%\n  mutate(DIV = ifelse(TEAM %in% western_conf, \"West\", \"East\")) %>%\n  mutate(DIV = as.factor(DIV))\n\ntitanic_mean <- mean(titanic_survival$age)\ntitanic_sd <- sd(titanic_survival$age)\n```\n\n\n## Thoughts about comparing groups\n\n- Faceted histograms are a reasonable display to show distributions by a categorical variable\n  + However these displays become hard to interpret when the number of levels in a category grows large\n- Much easier to interpret is side by side box plots\n- Box plots capture many important characteristics of a distribution into a summary display\n- Think carefully about how you treat outliers\n- Let's view data from the 2018-2019 NBA season\n\n## Two group comparison\n\n### NBA side-by-side histograms of points scored by W/L\n\n```{r nbacomparison-hist, exercise=TRUE}\nggplot(nbaboxscores, aes(x=PTS)) +\n  geom_histogram(fill=\"blue4\") +\n  labs(x=\"Points scored\", y=\"Count\") +\n  facet_wrap(~W.L)\n```\n\n### NBA boxplot comparison of points scored by W/L\n\n```{r nbacomparison-box, exercise=TRUE}\nggplot(nbaboxscores, aes(x=W.L,y=PTS)) + \n  geom_boxplot() + \n  labs(x=\"Result\", y=\"Points scored\")\n```\n\n### NBA boxplot comparison of points scored by W/L (better)\n\n```{r nbacomparison-betterbox, exercise=TRUE}\nggplot(nbaboxscores, aes(x=W.L,y=PTS)) + \n  geom_boxplot(fill=\"light blue\") + \n  labs(x=\"Result\", y=\"Points scored\") + \n  coord_flip()\n```\n\n## Many group comparison\n\n### NBA side-by-side histograms of points scored by team\n\n```{r nbateamcomparison-hist, exercise=TRUE}\nggplot(nbaboxscores, aes(x=PTS)) +\n  geom_histogram(fill=\"blue4\") +\n  labs(x=\"Points scored\", y=\"Count\") +\n  facet_wrap( ~ as.factor(TEAM))\n```\n\n### NBA boxplot comparison  of points scored by team (better)\n\n```{r nbateamcomparison-box, exercise=TRUE}\nggplot(nbaboxscores, aes(x=as.factor(TEAM),y=PTS, fill=DIV)) + \n  geom_boxplot() + \n  labs(x=\"Team\", y=\"Points scored\") +\n   scale_x_discrete(guide = guide_axis(n.dodge=2))\n```\n\n## Checking outliers - blocks\n\n### Outliers - blocks\n\n```{r blocksoutliers, exercise=TRUE}\nggplot(nbaboxscores, aes(x=BLK)) +\n  geom_histogram(fill=\"blue4\", binwidth=1) +\n  labs(x=\"Blocks\", y=\"Count\")\n```\n\n### Blocks > 13 - true outliers?\n\n```{r blocksoutlierscheck, exercise=TRUE}\nnbaboxscores %>% \n    filter(BLK > 13) %>%\n    select(TEAM, MATCH.UP, MIN, PTS, REB, PF, BLK)\n```\n\n```{r picker0, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n## Checking outliers - points\n\n### Outliers - points\n\n```{r pointsoutliers, exercise=TRUE}\nggplot(nbaboxscores, aes(x=PTS)) +\n  geom_histogram(fill=\"blue4\", binwidth=1) +\n  labs(x=\"Points\", y=\"Count\")\n```\n\n### Points > 150 - true outliers?\n\n```{r pointsoutlierscheck, exercise=TRUE}\nnbaboxscores %>% \n    filter(PTS > 150) %>%\n    select(TEAM, MATCH.UP, MIN, PTS, X3PM, PF)\n```\n\n```{r picker0-5, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n## In summary\n\n- Think about which kind of display is appropriate for comparing distributions\n- When conditioning on a categorical variable, boxplots are usually better\n- But boxplots lose information\n- Think carefully about omitting outliers\n- Outliers may reveal important information about your dataset!\n\n## Titanic passengers and the Normal distribution\n\n![Titanic](www/Titanic.jpg)\n\n### Dataset of passengers on the Titanic\n\n```{r datastructure, exercise=TRUE}\nstr(titanic_survival)\n```\n\n- What are your expectations for how each of the variable should be distributed?\n\n```{r picker1, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n- We are going to violate our first three rules:\n  1. Make a picture\n  2. Make a picture\n  3. Make a picture\n\n### Were the passenger ages normally distributed?\n\nTo answer that question, we need some information about the distribution\n\nRemember, our main information about distributions is:\n\n- Shape\n- Center\n- Spread\n\n- What information do you think we need to determine if `age` is normally distributed?\n\n```{r picker2, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Information about `age`\n\n- Standard deviation: `r round(sd(titanic_survival$age), digits=1)`\n- Mean: `r round(mean(titanic_survival$age), digits=1)`\n- Normal model: $N(\\mu, \\sigma) = N(`r round(mean(titanic_survival$age), digits=1)`, `r round(sd(titanic_survival$age), digits=1)`)$\n  + $\\mu$ is the theoretical mean\n  + $\\sigma$ is the theoretical standard deviation\n  + These values define the data generating process\n  + We only see some values of the data generating process, but if we saw infinite values, the mean would be $\\mu$ and the sd would be $\\sigma$\n  + More on this in the second half of class\n- How can we check normality using this information?\n\n```{r picker3, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n## Checking normality\n\n### Thinking about normality\n\n- We can check normality by comparing the quantiles of our data with that of the known quantiles of the normal distribution\n  + We know approximately 95% of the data lies within two standard deviations\n  + Therefore, 2.5% data with the lowest values lie outside of -2 standard deviations and 2.5% of data with the highest values lie outside of 2 standard deviations\n- Similarly, we know the same information for data within one standard deviation (16%, 68%, 16%)\n\n### Data within standard deviations\n\n```{r agesd, exercise=TRUE}\nnorm.dist <- rnorm(1000000)\n\nds.low <- density(norm.dist, from=min(norm.dist), to=-2)\nds.high <- density(norm.dist, from=2, to=max(norm.dist))\nds.low.mid <- density(norm.dist, from=min(norm.dist), to=-1)\nds.high.mid <- density(norm.dist, from=1, to=max(norm.dist))\n\nds.low.data <- data.frame(x = ds.low$x, y = ds.low$y)\nds.low.mid.data <- data.frame(x = ds.low.mid$x, y = ds.low.mid$y)\nds.high.data <- data.frame(x = ds.high$x, y = ds.high$y)\nds.high.mid.data <- data.frame(x = ds.high.mid$x, y = ds.high.mid$y)\n\ntwo.sd <- ggplot(data.frame(norm.dist), aes(norm.dist)) + \n  geom_density() + \n  geom_vline(xintercept=-2, color=\"dark red\") +\n  geom_vline(xintercept=2, color=\"dark red\") +\n  geom_area(data = ds.low.data, aes(x = x, y = y), fill=\"blue4\", color=\"blue4\") +\n  geom_area(data = ds.high.data, aes(x = x, y = y), fill=\"blue4\", color=\"blue4\") + \n  labs(x=\"Standard deviations\", y=\"Density\") + \n  scale_x_continuous(breaks=c(-3, -2, -1, 0, 1, 2, 3), limits=(c(-4, 4))) +\n  annotate(geom=\"text\", x=-3, y=0.1, label=\"~2.5% of the data\") +\n  annotate(geom=\"text\", x=3, y=0.1, label=\"~2.5% of the data\") +\n  annotate(geom=\"text\", x=0, y=0.1, label=\"~95% of the data\") \n  \none.sd <- ggplot(data.frame(norm.dist), aes(norm.dist)) +\n  geom_density() +\n  geom_vline(xintercept = -1, color=\"dark red\") +\n  geom_vline(xintercept = 1, color=\"dark red\") +\n  geom_area(data = ds.low.mid.data, aes(x = x, y = y), fill=\"blue4\", color=\"blue4\") +\n  geom_area(data = ds.high.mid.data, aes(x = x, y = y), fill=\"blue4\", color=\"blue4\") +\n  labs(x=\"Standard deviations\", y=\"Density\") + \n  scale_x_continuous(breaks=c(-3, -2, -1, 0, 1, 2, 3), limits=(c(-4, 4))) +\n  annotate(geom=\"text\", x=-2.5, y=0.2, label=\"~16% of the data\") +\n  annotate(geom=\"text\", x=2.5, y=0.2, label=\"~16% of the data\") +\n  annotate(geom=\"text\", x=0, y=0.2, label=\"~68% of the data\") \n\ngrid.arrange(two.sd, one.sd)\n```\n\n### Using quantiles to check normality\n\nRemember:\n\n- Standard deviation of `age`: `r round(sd(titanic_survival$age), digits=1)`\n- Mean of `age`: `r round(mean(titanic_survival$age), digits=1)`\n\nTherefore, \n\n- ~2.5% of the data should be below $\\mu - 2*\\sigma$ \n- In practical terms, $`r round(titanic_mean, digits=1)` - 2*`r round(titanic_sd, digits=1)` = `r round(titanic_mean - 2*titanic_sd, digits=1)`$\n  + Actual observation at ~2.5% cutoff: `quantile(titanic_survival$age, 0.0228)` = `r quantile(titanic_survival$age, 0.0228)`\n- Other cutoffs vs. actual:\n  + Predicted observation at ~16% cutoff: `r round(titanic_mean-titanic_sd, digits=1)`, Actual at ~16% cutoff: `r quantile(titanic_survival$age, 0.158)`  \n  + Predicted at ~84% cutoff: `r round(titanic_mean+titanic_sd, digits=1)`, Actual: `r quantile(titanic_survival$age, 0.842)`\n  + Predicted at ~97.5% cutoff: `r round(titanic_mean+2*titanic_sd, digits=1)`, Actual:`r quantile(titanic_survival$age, 0.9775)`\n\nBased on this information, what do you think the shape of the distribution is?\n\n```{r picker4, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n## Checking against the data\n\n### Histogram of ages from the data\n\n```{r agehist, exercise=TRUE}\nnorm.dist.titanic <- rnorm(1000000, mean=titanic_mean, sd=titanic_sd)\nnorm.dist.titanic <- data.frame(norm.dist.titanic)\n\nggplot(titanic_survival, aes(x=age)) + \n  geom_histogram(aes(y=..density..), fill=\"blue4\") +\n  geom_vline(xintercept=titanic_mean, color=\"dark red\") + \n  geom_density(data=norm.dist.titanic, aes(norm.dist.titanic), color=\"dark red\") +\n  labs(x=\"Age\", y=\"Count\") +\n  scale_x_continuous(limits=c(-10, 80))\n```\n\n### Normality and scaling\n\n- Note that normality does not depend on the size of the sd or the size of the mean\n- Could easily change the units to be months instead of years\n  + Mean would increase a lot\n  + Standard deviation would increase a lot\n  + However, amount of observations within each standard deviation would stay the same\n\n## Final thoughts on normality\n\n### When is the normal distribution useful?\n\n- When we know a data-generating process is normally distributed we don't even need to sample the population\n  + Can find out exactly how much data is between a certain number of standard deviations\n- When we expect a data-generating process to be normally distributed, can test for deviations from normality\n  + In the case of Titanic passengers, some parts of the distribution were more bunched up, others more spread out\n- A lot of our statistical techniques require or work better when the data is 'roughly' normal\n  + Will detail these in the coming weeks\n- We can transform our data to be closer to normal\n  + Note that transformations won't work if the data has multiple modes, can only correct skew\n\n### What transformation would be helpful for `age`?\n\n```{r agehist2, exercise=TRUE}\nggplot(titanic_survival, aes(x=age)) + \n  geom_histogram(fill=\"blue4\") +\n  labs(x=\"Age\", y=\"Count\")\n```\n\n```{r picker5, exercise=TRUE}\nsample(classroster$name, 1)\n```"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"learnr::tutorial":{"toc_depth":2}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Advanced Distributions.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.36","editor":"visual","theme":"cosmo","title":"Advanced Distributions","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"1/17/2022","runtime":"shiny_prerendered"},"extensions":{"book":{"multiFile":true}}}}}