{"title":"Comparing Groups","markdown":{"yaml":{"title":"Comparing Groups","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"2/23/2022","output":{"learnr::tutorial":{"toc_depth":2}},"runtime":"shiny_prerendered"},"headingText":"Comparing groups","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nknitr::opts_chunk$set(warning = FALSE)\nknitr::opts_chunk$set(message = FALSE)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(learnr)\n\ntheme_set(theme_classic())\n\nset.seed(8675309)\noptions(scipen=999)\n\ntitanic <- read.csv(\"www/titanic.survival.csv\")\nkc.house <- read.csv(\"www/kc.house.data.original.csv\")\nclassroster <- read.csv(\"www/classroster.csv\", fileEncoding=\"UTF-8-BOM\")\n\ntitanic <- titanic %>%\n  mutate(sex = as.factor(sex),\n         survived = factor(survived, levels=c(\"no\", \"yes\")))\n\npct.survived = mean(as.numeric(titanic$survied)) - 1\n\nrounded.mean.water <- round(mean(kc.house$price[kc.house$waterfront==1]), digits=0)\nrounded.mean.nowater <- round(mean(kc.house$price[kc.house$waterfront==0]), digits=0)\nrounded.sd.water <- round(sd(kc.house$price[kc.house$waterfront==1]), digits=0)\nrounded.sd.nowater <- round(sd(kc.house$price[kc.house$waterfront==0]), digits=0)\n\nn.water <- length(kc.house$price[kc.house$waterfront==1])\nn.nowater <- length(kc.house$price[kc.house$waterfront==0])\n\nrounded.diff <- round(mean(kc.house$price[kc.house$waterfront==1]) - mean(kc.house$price[kc.house$waterfront==0]), digits=0)\nrounded.SE <- round(sqrt(sd(kc.house$price[kc.house$waterfront==1])^2/n.water + sd(kc.house$price[kc.house$waterfront==0])^2/n.nowater), digits=0)\nrounded.t <- round((mean(kc.house$price[kc.house$waterfront==1]) - mean(kc.house$price[kc.house$waterfront==0]))/(sqrt(sd(kc.house$price[kc.house$waterfront==1])^2/n.water + sd(kc.house$price[kc.house$waterfront==0])^2/n.nowater)), digits=2)\nrounded.p <- 1-round(pt((mean(kc.house$price[kc.house$waterfront==1]) - mean(kc.house$price[kc.house$waterfront==0]))/(sqrt(sd(kc.house$price[kc.house$waterfront==1])^2/n.water + sd(kc.house$price[kc.house$waterfront==0])^2/n.nowater)), df=n.water), digits=6)\n\nt.test(kc.house$price[kc.house$waterfront==1], kc.house$price[kc.house$waterfront==0])\n\n```\n\n\n* What type of inference is a $t$ test?\n* A confidence interval for the difference between two means\n* The two-sample $t$ test: testing for the difference between two means\n* Experiments and causality\n\n## What type of inference is a $t$ test?\n\n### A review - descriptive vs. inference\n\n```{r inferencetable1}\ncnames <- c(\"Type of analysis\", \"Descriptive\", \"Inferential\")\ncol1 <- c(\"Univariate\", \"Univariate compared to theoretical expectation\", \"Comparing two variables\", \"Comparing many variables\")\ncol2 <- c(\"Histogram, bar chart\", \"QQ plot\", \"Scatterplot, two variable regression\", \"Multiple variable regression\")\ncol3 <- c(\"Confidence interval\", \"One proportion z test, one proportion t test\", \"Two proportion z test, two proportion t test\", \"Multiple variable regression\")\nreview.table <- data.frame(col1, col2, col3)\ncolnames(review.table) <- cnames\n\nkable(review.table) %>% \n  kable_styling()\n```\n\n### A review - one vs. two mean test\n\n```{r inferencetable2}\ncnames <- c(\"One mean test\", \"Two mean test\")\ncol1 <- c(\"Comparing the mean of your sample to some statement about the world\", \"Null hypothesis: based on some belief we have about the general population, i.e. students sleep 7.03 hours\")\ncol2 <- c(\"Comparing the mean of one part of your sample to another part of your sample\", \"Null hypothesis: no difference between groups\")\nreview.table <- data.frame(col1, col2)\ncolnames(review.table) <- cnames\n\nkable(review.table) %>% \n  kable_styling()\n```\n\n### Example\n\n```{r inferenceexample}\ncnames <- c(\"One mean test\", \"Two mean test\")\ncol1 <- c(\"$H_0$: Our sample mean of hours of sleep is the same as all students in the world\", \"$H_a$: Our sample mean is different than the world's population mean\")\ncol2 <- c(\"$H_0$: The sample mean of male students hours slept is the same as the mean of female students hours slept\", \"$H_a$: The sample mean of female students is different than the sample mean of male students\")\nreview.table <- data.frame(col1, col2)\ncolnames(review.table) <- cnames\n\nkable(review.table) %>% \n  kable_styling()\n```\n\n## A confidence interval for the difference between two means\n\n### Difference between means: standard error\n\n* Want to find the $SE$ for $\\bar{y}_1-\\bar{y}_2$\n* Start with theoretical properties:\n  + $SD(\\bar{y}_1-\\bar{y}_2) = \\sqrt{Var(\\bar{y}_1) + Var(\\bar{y}_2)}$\n  + $SD(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}$\n* Don't know the population $\\sigma$ for each subsample, so use the sample $SD$s as before\n  + $SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$\n  \n### Example\n\n* 658 male passengers on the Titanic; 135 survived\n* 388 female passengers on the Titanic; 292 survived\n* $SD(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{p_1q_1}{n_1} + \\frac{p_1q_2}{n_2}}$\n* $SE(0.205-0.753) = \\sqrt{\\frac{0.205\\times0.795}{658} + \\frac{0.753\\times0.247}{388}}$\n* $SE(0.205-0.753) = \\sqrt{0.00024 + 0.000479}$\n* $SE(0.205-0.753) = 0.0269$\n\n### Confidence interval\n\n* What is the 95% confidence interval of the *difference* in means?\n\n* $\\bar{y}_1-\\bar{y}_2\\pm Critical value \\times SE$\n\n* $-0.548\\pm z^* \\times 0.0269$\n\n* $-0.548\\pm 1.96 \\times 0.0269$\n\n* $-0.548\\pm 0.0528$\n\nWhat can you conclude from this - how can you state the results?\n\n```{r picker1, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### CI for the difference between two proportions/means\n\n* First find two-sample $z$/$t$ interval for the difference in means\n* Then apply two-sample $z$/$t$ test\n* Interval looks like others we have seen\n  + $\\bar{y}_1-\\bar{y}_2\\pm ME$\n  + $ME = t^*/z^*\\times SE(\\bar{y}_1-\\bar{y}_2)$\n* Uses the $z$ model (proportion) or Student’s $t$ model (mean)\n* The degrees of freedom for $t$ are complicated, so just use a computer\n\n### Sampling distribution for the difference between two means\n\n* When the conditions are met, the sampling distribution of the standardized sample difference between the means of two independent groups:\n  + $t = \\frac{(\\bar{y}_1-\\bar{y}_2) - (\\mu_1 - \\mu_2)}{SE(\\bar{y}_1-\\bar{y}_2)}$\n\n* Uses the Student's $t$ model\n\n* Degrees of freedom are found with a special formula\n\n* Think carefully here about what we are modeling\n\n### Assumptions\n\n* Independence assumption:\n  + Within each group, individual responses should be independent of each other.\n  + Knowing one response should not provide  information about other responses.\n* Randomization condition: \n  + If responses are selected with randomization, their independence is likely.\n* Independent Groups Assumption\n  + Responses in the two groups are independent of each other.\n  + Knowing how one group responds should not provide information about the other group.\n\n### Assumptions continued \n\n* Nearly normal condition\n  + Check this for both groups\n  + A violation by either one, violates the condition\n  + $n < 15$ in either group: should not use these methods if the histogram or Normal probability plot shows severe skewness \n  + $n$ closer to 40 for both groups: mildly skewed histogram is OK\n  + $n > 40$ for both groups: Fine as long as no extreme outliers or extreme skewness\n\n### Confidence interval formally\n\n* When the conditions are met, the confidence interval for the difference between means from two independent groups is\n\n  + $(\\bar{y}_1-\\bar{y}_2)\\pm t^*_{df}\\times SE(\\bar{y}_1-\\bar{y}_2)$\n\n  + where $SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$\n\n* Critical value $t^*_{df}$ depends on confidence level $C$\n\n## The two sample $t$ test: testing for the difference between two means\n\n### A two sample $t$ test for difference between means\n\n* Conditions same as two-sample t-interval\n  + $H_0: \\mu_1-\\mu_2 = \\Delta_0$ ($\\Delta_0$ usually $0$)\n  \n* When the conditions are met and the null hypothesis is true, use the Student’s $t$ model to find the $p$ value.\n  + $t = \\frac{(\\bar{y}_1-\\bar{y}_2) - \\Delta_0}{SE(\\bar{y}_1-\\bar{y}_2)}$\n  + $SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$\n  \n### Step by step example\n\n* Is there a difference in housing price depending on if the house is on the waterfront?\n  + Think $\\rightarrow$\n    - Plan: I have housing prices from many thousands of houses in King County, assumed to have been sampled randomly.\n    - Hypotheses\n    - $H_0: \\mu_w-\\mu_{notw}=0$\n    - $H_a: \\mu_w-\\mu_{notw}\\ne0$\n      \n### Step by step example\n\n* Think $\\rightarrow$\n  + Mean price on waterfront: `r rounded.mean.water`, mean price not on waterfront: `r rounded.mean.nowater`\n  + Model:  \n    - Randomization  Condition:  Subjects assigned to treatment groups randomly? \n    - Independent Groups Assumption:  Sampling method gives independent groups?\n    - Nearly normal condition:  Histograms are reasonably unimodal and symmetric? \n    - The assumptions and conditions are reasonable? \n\n```{r waterfrontpct, fig.height=4, exercise=TRUE}\nggplot(kc.house, aes(x=price)) +\n  geom_histogram(aes(y=..density..), fill=\"blue4\") + \n  scale_y_continuous(labels=scales::percent) +\n  labs(x=\"Price\", y=\"Percent cases\") +\n  facet_wrap(~factor(waterfront, labels=c(\"Not waterfront\", \"Waterfront\")))\n```\n\nAfter analyzing these assumptions , are we justified in using the Student’s t-model to perform a two-sample t-test?\n  \n```{r picker2, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Step by step example\n\n* Show\n  + Mechanics\n    - Mean price on waterfront: `r rounded.mean.water`\n    - Mean price not on waterfront: `r rounded.mean.nowater`\n    - SD on waterfront: `r rounded.sd.water`\n    - SD not on waterfront: `r rounded.sd.nowater`\n    - $n$ on waterfront: `r n.water`\n    - $n$ not on waterfront `r n.nowater`\n  \nWhat is the formula we should use in the next step?\n\n```{r picker3, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Step by step example\n\n* Show\n  + Mechanics\n    - solve for the SE: $SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$\n    - solve for the SE: $SE(\\bar{y}_1-\\bar{y}_2) = \\sqrt{\\frac{`r rounded.sd.water`}{`r n.water`} + \\frac{`r rounded.sd.nowater`}{`r n.nowater`}}$\n    - solve for the SE:\n    - find $t$ score: $t = \\frac{(\\bar{y}_1-\\bar{y}_2) - \\Delta_0}{SE(\\bar{y}_1-\\bar{y}_2)}$\n    - find $t$ score: $t = \\frac{(`r rounded.mean.water`-`r rounded.mean.nowater`) - 0}{`r rounded.SE`}$\n    - find $t$ score: $t = `r rounded.t`$\n    - find $p$ value: can use table with $df=`r n.water`$, $p = `r rounded.p`$\n  \n### Step by step example\n\nAlternatively, we can use the built-in $t$ test function\n\n* Show\n\n```{r waterttest, exercise=TRUE}\nt.test(kc.house$price[kc.house$waterfront==1], kc.house$price[kc.house$waterfront==0])\n```\n\nWhat can we conclude, based on the results of this $t$ test?\n\n```{r picker4, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Step by step example\n\n* Tell $\\rightarrow$\n  + Conclusion: the $p$ value = `r rounded.p` is less than the critical value\n  + If there were no difference in the mean prices, then a difference this large would occur 1 times in millions of times\n  + Too rare to believe happened by chance? **Yes**\n  + Reject $H_0$? **Yes**\n  + Conclude that houses with a view are more expensive than regular houses? **Yes**\n\n## Experiments\n\n### Independence\n\n* Independence assumption:\n  + Within each group, individual responses should be independent of each other.\n  + Knowing one response should not provide  information about other responses.\n* Randomization condition: \n  + If responses are selected with randomization, their independence is likely.\n* **Independent Groups Assumption**\n  + **Responses in the two groups are independent of each other.**\n  + **Knowing how one group responds should not provide information about the other group.**\n  \n### The importance of the counterfactual\n\n* For causal inference, one should ask the counter-factual question, for those who received “treatment”, what would have happened to them if they hadn't been treated?  \n\n* That is, we only observe one state of reality (had more vegetables), but we want to know the **DIFFERENCE** the treatment had on the person by asking what would have happened if the **DID NOT** receive the treatment\n\n### The importance of the counterfactual\n\n* More formally, we are interested in the difference the treatment has on the response variable (Health)\n\n* On a child ($y_1$)that did receive more vegetables, we want to consider the case of what would have happened if they had **NOT** had the vegetables and find the treatment effect\n  + Or, $y_1^t- y_1^c$ = treatment effect ($t$ denoting treatment; $c$ denoting control)\n  + Note that $y_1^t$ is observed, but $y_1^c$ is not.\n\n* The problem is one of missing data – how to estimate $y_1^c$?\n\n### Comparability problems\n\n* If subjects who receive treatment and those who do not are different in some important characteristics, we have selectivity bias – e.g. higher SES children were more likely to be in the vegetable treatment group  \n  + Violates the independent group assumption because if rich children are more likely to be in the \"eats vegetables\" group we know that the observed value of the response variable, $heath$, is likely to be higher\n  + Knowing which group they are in gives us some knowledge of what their observed value of $y$ will be\n\n* Often called “omitted variable bias.”\n\n* Big problem in observational studies – many variables are probably not present that we’d like to know\n\n* What are some omitted variables that might bias our finding that houses on the water have a higher price than houses not on the water? \n\n```{r picker5, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Experiments\n\n* Experiments solve the omitted variable bias\n* Random assignment of treatment and control status ensures that subjects differ **ON AVERAGE** only in the treatment they receive\n* We can then compute the Average Treatment Effect (ATE) of being in the control vs. treatment\n* A $t$ test between treatment and control group will therefore be accurate\n* In observational studies, it is very rare to be able to guarantee that assignment to the two groups are independent of the response variable. \n  + If there are important other omitted variables that influence assignment to the two groups, need to control for these omitted variables via a multiple regression \n\n### Drawbacks of experiments\n\n* Lack of generalizability – Often done on college students or in contrived settings (external validity)\n* Cost – very expensive to run a full experiment\n* Ethics – why shouldn’t we give positive treatments to everyone?\n* Mechanically complicated\n  + Difficult to ensure proper randomization \n  + Difficult to design appropriate treatments\n  + Difficult to develop appropriate measurements \n\n### Multiple regression\n\n* Multiple Regression\n  + Attempts to control for, or estimate the treatment effect, for each variable included, **INDEPENDENT** of the other variables\n  + How sure are we of the treatment effect? \n  + $t$ test of the slope of the regression line\n  + Null hypothesis is that treatment variable (\"eats vegetables\") makes no difference on response variable\n  + If slope is non-zero, it indicates that differences in treatment produce differences in response variable (increase education $\\rightarrow$ increase in wages)"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"learnr::tutorial":{"toc_depth":2}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Comparing Groups.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.36","editor":"visual","theme":"cosmo","title":"Comparing Groups","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"2/23/2022","runtime":"shiny_prerendered"},"extensions":{"book":{"multiFile":true}}}}}