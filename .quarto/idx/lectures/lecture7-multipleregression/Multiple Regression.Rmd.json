{"title":"Multiple Regression","markdown":{"yaml":{"title":"Multiple Regression","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"2/7/2022","output":{"learnr::tutorial":{"toc_depth":2}},"runtime":"shiny_prerendered"},"headingText":"Multiple regression","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nknitr::opts_chunk$set(warning = FALSE)\nknitr::opts_chunk$set(message = FALSE)\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(gridExtra)\nlibrary(broom)\nlibrary(visreg)\nlibrary(learnr)\nlibrary(kableExtra)\n\ntheme_set(theme_classic())\nset.seed(20200915)\n\nkc.housing <- read.csv(\"www/kc.house.data.all.csv\")\nwages <- read.csv(\"www/us.dol.wages.csv\")\nclassroster <- read.csv(\"www/classroster.csv\", fileEncoding=\"UTF-8-BOM\")\noptions(scipen=999)\n\nmod <- lm(data=kc.housing, price ~ sqft_living + sqft_lot)\nkc.housing.aug <- augment(mod, kc.housing)\n```\n\n\n* Basic interpretation\n* Assumptions\n* Checks\n* Indicator variables\n* Interaction terms\n\n```{r seattlehouseimg, out.width = \"400px\", fig.cap=\"Seattle houses^[Credit to: https://crosscut.com/opinion/2020/11/washington-state-housing-question-and-answer]\"}\nknitr::include_graphics(\"www/seattlehousing.png\")\n```\n\n\n\n## Basic multiple regression interpretation\n\n### House prices\n\n```{r livingpricemod, exercise=TRUE}\nsummary(lm(data=kc.housing, price ~ sqft_living), digits=3)\n```\n\n### When linear regression is not enough\n\n* $R^2 = 0.49%$ for `sqft_livingspace` and `price`\n\n* 49% of the variation in Price is accounted for\n\n* What about the other 51%?\n\n* Could include other lurking variables such as size of the lot a house is on - more land, higher cost right?\n\n* A regression with two or more predictor variables is called a multiple regression.\n\n### What is multiple regression?\n\n* For a simple regression, with one independent variable, the least squares line makes residuals as small as possible.\n\n* For multiple regression, the regression equation still makes the residuals as small as possible.\n\n* No longer trying to create a line though – instead a multidimensional hyperplane!\n\n* Calculations difficult.  \n\n### Check `sqft_lot` and `price`\n\n```{r lotpricemod, exercise=TRUE}\nsummary(lm(data=kc.housing, price ~ sqft_lot), digits=3)\n```\n\nWhat do you think will happen to the coefficient on `sqft_lot` when we add `sqft_living`?\n\n```{r picker1, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Adding both terms\n\n```{r bothpricemod, exercise=TRUE}\nsummary(lm(data=kc.housing, price ~ sqft_living + sqft_lot), digits=3)\n```\n\n### The results\n\n* $R^2=0.49$\n\n* $s_e=261400$ \n\n* Coefficient:  \n  + $price = −441900 + 283.1sqft\\_livingspace - 28.9sqft\\_lot$\n\nHow would you interpret this model and the diagnostic statistics?\n\n```{r picker2, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Further investigation\n\n```{r livingvslot, exercise=TRUE}\nggplot(kc.housing, aes(x=sqft_lot, y=sqft_living)) +\n  geom_point() + \n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(x=\"Sqft lot size\", y=\"Sqft living space\") + \n  scale_x_continuous(labels = scales::comma) + \n  scale_y_continuous(labels = scales::comma) \n```\n\n## What is different in multiple regression?\n\n* Meaning of coefficients has changed in a subtle way.\n\n* Is an extraordinarily versatile calculation, underlying many widely used statistics methods.\n\n* Offers a glimpse into statistical models that use more than two quantitative variables. \n\n* Models that use several variables can be a big step toward realistic and useful modeling of complex phenomena and relationships\n\n### Multiple regression - coefficients\n\n* Can’t assume coefficients will stay the same\n\n* Coefficients change\n\n* Often in unexpected ways\n\n* Even changing signs\n\n* Be alert for a change in value\n\n* Be alert for a change in meaning\n\n### Multiple regression model\n\n* No simple relationship between $y$ and $x_j$, yet $b_j$ in a multiple regression may be quite different from zero\n\n* Strong two-variable relationship between $y$ and $x_j$, yet $b_j$ in a multiple regression to be almost zero\n\n* Strong two-variable relationship between $y$ and $x_j$, yet $b_j$ an be opposite in sign in a multiple regression\n\n* Easy to extend the model with more predictors\n\n* Residuals $e = y - \\hat{y}$\n\n## Assumptions\n\n### Three key assumptions\n\n* Linearity assumption (straight enough condition)\n\n* No pattern in residuals (outliers, straight enough condition)\n\n* Equal variance assumption (does the plot thicken?)\n\n### Linearity assumption\n\n* Straight Enough Condition\n  + We must check the scatterplot for each of the predictor variables vs. the response variable\n\n  + Do not need the scatterplots to show any discernible slope, but should be reasonably straight\n\n  + Cannot have bends, or other nonlinearity\n\n  + Can be easier to look at the plot of residuals\n\n```{r scatterplots, exercise=TRUE}\nlivingsp <- ggplot(kc.housing, aes(x=sqft_living, y=price)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(x=\"Sqft living space\", y=\"Price\") +\n  scale_x_continuous(labels = scales::comma) + \n  scale_y_continuous(labels = scales::comma) \n\nlot <- ggplot(kc.housing, aes(x=sqft_lot, y=price)) +\n  geom_point() + \n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(x=\"Sqft lot size\", y=\"Price\") + \n  scale_x_continuous(labels = scales::comma) + \n  scale_y_continuous(labels = scales::comma) \n\ngrid.arrange(livingsp, lot)\n```\n\n### Check the residual\n\n* Errors have a distribution that is:\n  + Unimodal\n  + Symmetric\n  + Without outliers\n\n* Look at histogram of residuals\n\n* Assumption is less important as sample size increases\n\n```{r residhist, exercise=TRUE}\nmod <- lm(data=kc.housing, price ~ sqft_living + sqft_lot)\nkc.housing.aug <- augment(mod, kc.housing)\n\nggplot(kc.housing.aug, aes(x=.fitted)) + \n  geom_histogram(fill=\"blue4\") + \n  labs(x=\"Residuals\", y=\"Count\") +  \n  scale_x_continuous(labels = scales::comma)\n```\n\n### Equal variance assumption\n\n* Same variability of the errors for all values of each predictor\n\n* Does the Plot Thicken? Condition: the spread around the line must be nearly constant.\n\n* Be alert for “fan” shaped pattern\n\n* Or other tendency for variability to grow or shrink in one part of the scatterplot\n\n```{r residscatter, exercise=TRUE}\nggplot(kc.housing.aug, aes(x=.fitted, y=.resid)) + \n  geom_point() + \n  geom_hline(yintercept=0, color=\"red\") +\n  labs(x=\"Fitted values\", y=\"Residuals\") +  \n  scale_x_continuous(labels = scales::comma) + \n  scale_y_continuous(labels = scales::comma) \n```\n\n### Decision loop\n\n* Straight Enough Condition: scatterplots of y-variable against each x-variable\n    + If straight enough, fit multiple regression model\n\n* How were data collected? Random? Represent identifiable population? Time? check independence\n\n* Find the residuals and predicted values.\n\n* Scatterplot of the residuals against predicted values: patternless, no bends, no thickening\n\n* Histogram of residuals: unimodal, symmetric, without outliers\n\n* If conditions check out, interpret regression model, and make predictions.\n\n## Partial residual plots\n\nOne of the best ways to check the linearity condition is with a partial residual plot. This plot displays the relationship between the predictor variable and the response variable after removing all of the variance of the other variables in the explanatory variable.\n\n### How to check variables individually\n\n* Checked overall equation for weirdness in residuals\n\n* What about each individual variable’s contribution to the regression?\n\n* Partial residual plot to the Rescue!\n\n* Look at plot to judge whether its form is straight enough.\n\n### Partial residual plots\n\n```{r partialresids, exercise=TRUE}\neff1 <- visreg(mod, \"sqft_living\", gg=TRUE) +\n  labs(x=\"Sqft living space\",  y=\"Price\") +\n  scale_x_continuous(labels = scales::comma) + \n  scale_y_continuous(labels = scales::comma) \n\neff2 <- visreg(mod, \"sqft_lot\", gg=TRUE) + \n  labs(x=\"Sqft lot size\", y=\"Price\") +\n  scale_x_continuous(labels = scales::comma) + \n  scale_y_continuous(labels = scales::comma) \n\ngrid.arrange(eff1, eff2)\n```\n\n### Meaning of a partial residual plot\n\n* Least squares line fit to plot has slope equal to the coefficient the plot illustrates.\n\n* Residuals are same as final residuals of multiple \nregression\n  + Judge strength of estimation of the plot’s coefficients\n\n* Outliers seen the same as they would appear in a simple scatterplot\n\n* Direction corresponds to the sign of multiple regression coefficient\n\n## Indicator variables\n\n### Wages\n\n* Indicator variables are for when we want to include categorical variables in our regression\n    + In a union vs. not in a union\n    + Often coded at 1=true 0=false, but that’s just convention, doesn’t really matter (remember, units don’t matter for regression)\n\n* Regression equation\n    + $wages = b_0 + b_1exp + b_2union$\n \n### Wages\n\n```{r wagesmod, exercise=TRUE}\nsummary(lm(data=wages, wage ~ exp + factor(union)), digits=3)\n```\n\n### Slopes of lines\n\n```{r wagesplot, exercise=TRUE}\nggplot(wages, aes(x=exp, y=wage)) +\n  geom_point(aes(color=factor(union))) +\n  geom_abline(slope=8.2430, intercept=747.5634, color=\"#F8766D\") +\n  geom_abline(slope=8.2430, intercept=669.85, color=\"#00BFC4\") +\n  labs(x=\"Years experience\", y=\"Wage in dollars\", color=\"Union member?\")\n```\n\n### Predict some values\n\n* Equation: $wages = 747.5634 + 8.2430exp + -77.7134union$\n\n```{r wagestableind}\nexp.table <- c(\"Experience\", \"5\", \"10\", \"15\", \"20\")\nin.union <- c(\"In union\", \" \", \" \", \" \", \" \")\nnot.union <- c(\"Not in union\", \" \", \" \", \" \", \" \")\n\ndf <- NULL\n\ndf <- rbind(df, exp.table)\ndf <- rbind(df, in.union)\ndf <- rbind(df, not.union)\n\nkable(df, row.names=FALSE) %>% \n  kable_styling()\n```\n\n```{r picker3, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n## Interaction terms\n\n### Interaction effects\n\n* What if lines are not roughly parallel?\n\n* Indicator variable that is 0 or 1 shifts line up or down.\n    + Can’t change slope\n    + Works only when same slope just different intercepts\n\n### Adjusting for different slopes\n\n* Introduce another constructed variable\n\n* The one is the product of an indicator variable and the predictor variable\n\n* Coefficient of this constructed **interaction** term gives adjustment to slope, $b_1$, to be made for the individuals in the indicated group.\n\n### Adjusting for different slopes\n\n```{r interactionmodel, exercise=TRUE}\nsummary(lm(data=wages, wage ~ exp + factor(union) + factor(union) * exp, digits=3))\n```\n\n### Different slopes for wages\n\n```{r interactionplot, exercise=TRUE}\nggplot(wages, aes(x=exp, y=wage, color=factor(union))) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  labs(x=\"Years experience\", y=\"Wage in dollars\", color=\"Union member?\")\n```\n\n### Predict some values\n\n* Equation: $wages = 710.7896 + 10.1421exp + 28.9884union + -5.2755union*exp$\n\n```{r wagestableint, exercise=TRUE}\nexp.table <- c(\"Experience\", \"5\", \"10\", \"15\", \"20\")\nin.union <- c(\"In union\", \" \", \" \", \" \", \" \")\nnot.union <- c(\"Not in union\", \" \", \" \", \" \", \" \")\n\ndf <- NULL\n\ndf <- rbind(df,exp.table)\ndf <- rbind(df, in.union)\ndf <- rbind(df, not.union)\n\nkable(df, row.names=FALSE) %>% \n  kable_styling()\n```\n\n```{r picker4, exercise=TRUE}\nsample(classroster$name, 1)\n```"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"learnr::tutorial":{"toc_depth":2}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Multiple Regression.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.36","editor":"visual","theme":"cosmo","title":"Multiple Regression","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"2/7/2022","runtime":"shiny_prerendered"},"extensions":{"book":{"multiFile":true}}}}}