{"title":"Confidence Intervals - Means","markdown":{"yaml":{"title":"Confidence Intervals - Means","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"2/14/2022","output":{"learnr::tutorial":{"toc_depth":2}},"runtime":"shiny_prerendered"},"headingText":"Confidence intervals - means","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nknitr::opts_chunk$set(warning = FALSE)\nknitr::opts_chunk$set(message = FALSE)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(scales)\nlibrary(learnr)\n\ntheme_set(theme_classic())\n\nset.seed(8675309)\noptions(scipen=999)\n\nmean.multiple.samples <- function(numdraws, numsamples, variable) { \n     meanvector <- c() \n     meanonesample <- 0 \n     for (i in 1:numsamples) { \n\t   meanonesample <- mean(sample(variable, numdraws, replace=TRUE)) \n         meanvector[i] <- meanonesample \n     } \n     meanvector \n}\n\nkc.house <- read.csv(\"www/kc.house.data.original.csv\")\nclassroster <- read.csv(\"www/classroster.csv\", fileEncoding=\"UTF-8-BOM\")\n\nsample20 <- sample(kc.house$price, 20, replace=TRUE)\n\nupper.bound <- round((mean(sample20) + (qt(0.975,df=19) * sd(sample20) / sqrt(19))), digits=0)\nlower.bound <- round((mean(sample20) - (qt(0.975,df=19) * sd(sample20) / sqrt(19))), digits=0)\n```\n\n\n* The central limit theorem\n* A confidence interval for the mean\n* Interpreting confidence intervals\n* Picking our interval up by our bootstraps\n* Thoughts about confidence intervals\n\n## House price revisited\n\n* Prices in King County Houses:\n  + `r as.numeric(kc.house %>% summarize(n=n()))` houses\n  + Highly right skewed\n  + Can define this as the entire population\n  + Prices are quantitative\n\n### House price graph\n\n```{r pricedensity, exercise=TRUE}\nggplot(kc.house, aes(x=price)) +\n  geom_histogram(aes(y=..density..),  fill=\"lightblue\", color=\"darkblue\") +\n  labs(x=\"Price in USD\", y=\"Density\") +\n  scale_x_continuous(labels=comma) +\n  scale_y_continuous(labels=comma)\n```\n\n### Distribution\n\n* Distribution:\n  + Min: `r round(min(kc.house$price), digits=0)`\n  + Q1: `r round(quantile(kc.house$price, probs=0.25), digits=0)`\n  + Med: `r round(median(kc.house$price), digits=0)`\n  + Q3: `r round(quantile(kc.house$price, probs=0.75), digits=0)`\n  + Max: `r round(max(kc.house$price), digits=0)`\n  + Mean: `r round(mean(kc.house$price), digits=0)`\n  + SD: `r round(sd(kc.house$price), digits=0)`\n\n* Highly right skewed\n\n* SD almost as large as the median\n\n* If a distribution looks like this, is it possible to take a sample and generate an accurate mean and confidence interval estimate?\n\n```{r picker1, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n## The central limit theorem\n\n* The Central Limit Theorem\n  + The sampling distribution of any mean becomes nearly Normal as the sample size grows.\n\n* **Requirements**\n  + Observations independent\n  + Randomly collected sample\n\n* The sampling distribution of the means is close to Normal if **either**:\n  + Large sample size\n  + Population close to Normal\n\n### Samples = 100, $n$ = 200\n\n```{r pricesmall, exercise=TRUE}\nhouses <- mean.multiple.samples(200, 100, kc.house$price)\n\nggplot(data.frame(houses), aes(x=houses)) +\n  geom_histogram(aes(y=..density..),  fill=\"lightblue\", color=\"darkblue\") +\n  geom_density(color=\"darkred\", size=1) +\n  labs(x=\"Price in USD\", y=\"Density\") +\n  scale_x_continuous(labels=comma) +\n  scale_y_continuous(labels=comma)\n```\n\n### Samples = 1000, $n$ = 200\n\n```{r pricemedium, exercise=TRUE}\nhouses <- mean.multiple.samples(200, 1000, kc.house$price)\n\nggplot(data.frame(houses), aes(x=houses)) +\n  geom_histogram(aes(y=..density..),  fill=\"lightblue\", color=\"darkblue\") +\n  geom_density(color=\"darkred\", size=1) +\n  labs(x=\"Price in USD\", y=\"Density\") +\n  scale_x_continuous(labels=comma) +\n  scale_y_continuous(labels=comma)\n```\n\n### Samples = 100000, $n$ = 200\n\n```{r pricelarge, exercise=TRUE}\nhouses <- mean.multiple.samples(200, 100000, kc.house$price)\n\nggplot(data.frame(houses), aes(x=houses)) +\n  geom_histogram(aes(y=..density..),  fill=\"lightblue\", color=\"darkblue\") +\n  geom_density(color=\"darkred\", size=1) +\n  labs(x=\"Price in USD\", y=\"Density\") +\n  scale_x_continuous(labels=comma) +\n  scale_y_continuous(labels=comma)\n```\n\n## Sampling distribution shape\n\n* As number of samples taken goes to infinity, shape of the **sampling** distribution becomes more clearly normally shaped\n\n* Doesn't matter the shape of the underlying distribution except for a very few exceptions\n\n* How about holding samples fixed and changing $n$ in our sample of a skewed distribution?\n\n### $n$ = 10\n\n```{r sample10, exercise=TRUE}\nhouses <- mean.multiple.samples(10, 100000, kc.house$price)\n\nggplot(data.frame(houses), aes(x=houses)) +\n  geom_histogram(aes(y=..density..),  fill=\"lightblue\", color=\"darkblue\") +\n  geom_density(color=\"darkred\", size=1) +\n  labs(x=\"Price in USD\", y=\"Density\") +\n  scale_x_continuous(labels=comma) +\n  scale_y_continuous(labels=comma)\n```\n\n### $n$ = 25\n\n```{r sample25, exercise=TRUE}\nhouses <- mean.multiple.samples(25, 100000, kc.house$price)\n\nggplot(data.frame(houses), aes(x=houses)) +\n  geom_histogram(aes(y=..density..),  fill=\"lightblue\", color=\"darkblue\") +\n  geom_density(color=\"darkred\", size=1) +\n  labs(x=\"Price in USD\", y=\"Density\") +\n  scale_x_continuous(labels=comma) +\n  scale_y_continuous(labels=comma)\n```\n\n### $n$ = 50\n\n```{r sample50, exercise=TRUE}\nhouses <- mean.multiple.samples(50, 100000, kc.house$price)\n\nggplot(data.frame(houses), aes(x=houses)) +\n  geom_histogram(aes(y=..density..),  fill=\"lightblue\", color=\"darkblue\") +\n  geom_density(color=\"darkred\", size=1) +\n  labs(x=\"Price in USD\", y=\"Density\") +\n  scale_x_continuous(labels=comma) +\n  scale_y_continuous(labels=comma)\n```\n\n\n### $n$ = 100\n\n```{r sample100, exercise=TRUE}\nhouses <- mean.multiple.samples(100, 100000, kc.house$price)\n\nggplot(data.frame(houses), aes(x=houses)) +\n  geom_histogram(aes(y=..density..),  fill=\"lightblue\", color=\"darkblue\") +\n  geom_density(color=\"darkred\", size=1) +\n  labs(x=\"Price in USD\", y=\"Density\") +\n  scale_x_continuous(labels=comma) +\n  scale_y_continuous(labels=comma)\n```\n\n### Central limit theorem formally\n\n* When a random sample is drawn from any population with mean $\\mu$ and standard deviation $\\sigma$, its sample mean, $\\bar{y}$, has a sampling distribution with the same mean    but whose *standard deviation* is $\\frac{\\sigma}{\\sqrt{n}}$ and we write $\\sigma(\\bar{y})=SD(\\bar{y})=\\frac{\\sigma}{\\sqrt{n}}$\n\n* No matter what population the random sample comes from, the shape of the sampling distribution is approximately Normal as long as the sample size is large enough. \n\n* The larger the sample used, the more closely the Normal approximates the sampling distribution for the mean. \n\n* Practically, $n$ does not have to be very large for this to work in most cases\n\n### Practical issue with finding the sampling distribution sd\n\n* We almost never know $\\sigma$\n \n* Natural thing is to use $\\hat{sd_{sample}}$\n\n* With this, we can estimate the **sampling distribution** SD with SE: \n  + $SE(\\bar{y})=\\frac{s}{\\sqrt{n}}$\n  \n* This formula works well for large samples, not so much for small\n  + Problem: too much variation in the sample SD from sample to sample\n\n* For smaller $n$, need to turn to Gosset and a new family of models depending on sample size\n\n## A confidence interval for the mean\n\n### Gosset the brewer\n\n![Guiness](www/guiness.jpg)\n\n### Gosset\n![Gosset](www/gosset.png)\n\n### What Gosset discovered\n\n* At Guinness, Gosset experimented with beer.\n\n* The Normal Model was not right, especially for small samples.\n\n* Still bell shaped, but details differed, depending on $n$\n\n* Came up with the “Student’s $t$ Distribution” as the correct model\n\n### A practical sampling distribution model\n\n* When certain assumptions and conditions are met, the standardized sample mean is:\n\n$t=\\frac{\\bar{y}-\\mu}{SE(\\bar{y})}$\n\n* The *t* score indicates that the result should be interpreted by a Student's $t$ model with $n-1$ degrees of freedom\n\n* We can estimate the standard deviation of the **sampling distribution**  by:\n\n$SE(\\bar{y}) = \\frac{s}{\\sqrt{n}}$\n\n### Degrees of freedom\n\n* For every sample size $n$, there is a different Student's $t$ distribution\n\n* Degrees of freedom: $df=n-1$\n\n* Similar to the $n-1$ calculation for sample standard deviation\n\n* Reason for this is a bit complicated, at this point can just remember to specify $t$ distribution with $n-1$\n\n### Student's $t$\n\n```{r, echo=FALSE, out.width=\"400px\", fig.cap=\"Student's $t$ for different $df$\"}\nknitr::include_graphics(\"www/studentst.png\")\n```\n\n### One sample $t$ interval for the mean\n\n* When the assumptions are met, the confidence interval for the mean is:\n\n$\\bar{y} \\pm t_{n-1}\\times SE(\\bar{y})$\n\n* The critical value, $t^*_{n-1}$, depends on the confidence interval, $C$, and the degrees of freedom $n-1$\n\n### Example: A one sample $t$ interval for the mean\n\n* Price from one sample in King County\n\n```{r detailssample20, exercise=TRUE}\n#sample20 <- sample(kc.house$price, 20, replace=TRUE)\n\nround(mean(sample20), digits=0)\nround(sd(sample20), digits=0)\nqt(0.975,df=19)\n```\n\n### Average house price\n\n* $\\bar{y}\\pm t^*_{19} \\times SE(\\bar{y})$\n\n* $`r round(mean(sample20), digits=0)`\\pm `r round(qt(0.975,df=19), digits=2)` \\times SE(\\frac{`r round(sd(sample20), digits=0)`}{\\sqrt{(20-1)}})$\n\n* $`r round(mean(sample20), digits=0)`\\pm `r round(qt(0.975,df=19), digits=2)` \\times `r round((sd(sample20) / sqrt(19)), digits=2)`$\n\n* $[`r lower.bound` - `r upper.bound`]$\n\nWhat is the right way to talk about this confidence interval?\n\n```{r picker2, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n### Thoughts about $z$ and $t$\n\n* The Student’s t distribution:\n  + Is unimodal.\n  + Is symmetric about its mean.\n  + Bell-shaped\n  \n* Samller values of $df$ have longer tails and larger standard deviation than the Normal.\n\n* As $df$ increase, look more and more like Normal.\n\n* Is needed because we are using s as an estimate for $\\sigma$\n\n* If you happen to know $\\sigma$, which almost never happens, use the Normal model and not Student’s $t$\n\n* As $n$ becomes larger, still safe to use the $t$ distribution because it basically turns into the normal distribution\n\n### Assumptions and conditions\n\n* Independence Assumption\n  + Data values should be mutually independent\n  + Example: weighing yourself every day\n\n* Randomization Condition: The data should arise from a random sample or suitably randomized experiment.\n  + Data from SRS almost surely independent\n  + If doesn’t satisfy Randomization Condition, think about whether values are independent and whether sample is representative of the population.\n\n### Assumptions and conditions\n\n* Normal Population Assumption\n  + Nearly Normal Condition: Distribution is unimodal and symmetric.\n  + Check with a histogram.\n  + $n < 15$:  data should follow a normal model closely. If outliers or strong skewness, don’t use $t$-methods\n  + $15 < n < 40$:  $t$-methods work well as long as data are unimodal and reasonably symmetric.\n  + $n > 40$:  $t$-methods are safe as long as data are not extremely skewed.\n  + Similar to the rule for proportions that must have somewhat even distribution of yeses and noes\n\n### Example: Checking Assumptions and Conditions for Student’s $t$\n\n* Price of housing in King County\n\n  + Independence Assumption: **Yes**\n  \n  + Nearly Normal Condition: **No**\n  \n```{r sample20plot, exercise=TRUE}\nggplot(data.frame(sample20), aes(x=sample20)) +\n  geom_histogram( fill=\"lightblue\", color=\"darkblue\", bins=10) +\n  labs(x=\"Price in USD\", y=\"Count\")\n```\n\n## Interpreting confidence intervals\n\n### What not to say\n\nDon’t say:  \n\n  + “95% of the price of houses in King County is between $`r lower.bound` and $`r upper.bound`.”\n    - The CI is about the *mean* price, not about the *individual* houses.\n\n  + “We are 95% confident that a randomly selected house price will be between $`r lower.bound` and $`r upper.bound`.”\n    - Again, we are concerned here with the mean, not *individual* houses\n\n### What not to say continued\n\nDon’t Say  \n\n  + “The mean price is $`r round(mean(sample20), digits=0)` 95% of the time.”\n    - The population mean never changes.  Only sample means vary from sample to sample.\n    \n  + “95% of all samples will have a mean price between $`r lower.bound` and $`r upper.bound`.”\n    - This interval does not set the standard for all other intervals. This interval is no more likely to be correct than any other.\n\n### What you should say\n\nDo Say  \n\n  + “I am 95% confident that the true mean price is between $`r lower.bound` and $`r upper.bound`.”\n\n    - Technically: “95% of all random samples will produce intervals that cover the true value.” \n\nThe first statement is more personal and less technical.\n\n## Bootstrapping\n\n### Picking our interval up by our bootstraps\n\n```{r pricebootstrap, exercise=TRUE}\nhouses <- mean.multiple.samples(100, 100000, kc.house$price)\n\nggplot(data.frame(houses), aes(x=houses)) +\n  geom_histogram(aes(y=..density..), fill=\"lightblue\", color=\"darkblue\") +\n  geom_density(color=\"darkred\", size=1) +\n  labs(x=\"Price in USD\", y=\"Density\", title=\"Distribution of means where n=100\") +\n  scale_x_continuous(labels=comma) +\n  scale_y_continuous(labels=comma)\n```\n\n### Keep in mind\n\n* The confidence interval (unlike the sampling distribution) is centered at $\\bar{y}$ rather than at $\\mu$.\n\n* We need to know how far to reach out from $\\bar{y}$, so we need to estimate the population standard deviation. Estimating $sigma$ means we need to refer to Student’s $t$-models. \n* Using Student’s $t$-requires the assumption that the underlying data follow a Normal model. \n  - Practically, we need to check that the data distribution **of our sample** is at least unimodal and reasonably symmetric, with no outliers for $n<100$. \n\n### Bootstrapping\n\nProcess:  \n\n* We have a random sample, representative of population.\n* Make copies and build a pseudo-population\n* Sample repeatedly from this population\n* Find means\n* Make a histogram\n* Observe how means are distributed and how much they vary\n\n### Bootstrapping\n\n```{r bootstrapcompare, exercise=TRUE}\n# True sampling distribution\nsamplingdist.means <- mean.multiple.samples(100, 100000, kc.house$price)\n\n# Bootstrapped sampling distribution\nsurvey <- sample(kc.house$price, 100) \nbootstrapped.means <- mean.multiple.samples(100, 100000, survey)\n\nquantile(bootstrapped.means, probs=c(0.025,0.975))\nquantile(samplingdist.means, probs=c(0.025,0.975))\n```\n\nHow does this compare to the confidence interval calculated by classical means?\n\n```{r picker3, exercise=TRUE}\nsample(classroster$name, 1)\n```\n\n## Thoughts about confidence intervals\n\n### Confidence intervals - what's important\n\n* It’s not their precision.\n* Our specific confidence interval is random by nature\n* Changes with the sample\n* Important to know how they are constructed\n* Need to check assumptions and conditions\n* Contains our best guess of the mean\n* And how precise we think that guess is\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"learnr::tutorial":{"toc_depth":2}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"Confidence Intervals - Means.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.36","editor":"visual","theme":"cosmo","title":"Confidence Intervals - Means","subtitle":"DKU Stats 101 Spring 2022","author":"Professor MacDonald","date":"2/14/2022","runtime":"shiny_prerendered"},"extensions":{"book":{"multiFile":true}}}}}